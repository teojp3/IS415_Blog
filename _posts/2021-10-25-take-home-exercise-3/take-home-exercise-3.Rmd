---
title: "Take Home Exercise 3"
description: |
    A study on the resale prices of public housing in Singapore, through the use of Hedonic Pricing Models.
author:
  - name: Teo Jun Peng
    url: https://teojp3-is415.netlify.app/
date: 2021-11-06
output:
  distill::distill_article:
      self_contained: false
      toc: true
      toc_depth: 3
      toc_float: true
---

```{r setup, include=FALSE, eval=TRUE, echo=TRUE, message=FALSE, error=FALSE, fig.retina=3}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

# 1. Introduction

Housing is an essential component of household wealth worldwide, with the price of housing properties being objectively determined by many factors. Some of these factors are global in nature, such as the general economy or inflation rate of a country.

Alternatively, the affecting factors can be further segmented into structural and locational factors. Structural factors are variables relating to the properties themselves such as the size, fitting, and tenure of the property. Meanwhile, locational factors are variables relating to the surrounding neighbourhood of the properties such as proximity to facilities like childcare centres, public transport services and shopping centres.

We will be conducting this investigation via a local case-study context. In Singapore, housing prices are infamous for being expensive, [ranking only behind Hong Kong which is the world's most expensive housing market.](https://www.cbre.com.sg/about/media-centre/singapore-remains-the-2nd-most-expensive-housing-market-in-the-world-after-hong-kong)  

<p> Therefore, we will be trying to find out exactly **which factors actually affects the pricing of public housing**, is it because of Singapore's highly-developed economy? Or is it more of the local structural and locational factors that affects the pricing more? &#129488; </p>

<p>  Join me in this study to **uncover the truth behind what constitutes or justifies a high property price in Singapore** &#128200; </p>

## 1.1 Objectives and Methods

In this study, we will be building **Hedonic Pricing Models** to explain the relevant factors that affects resale prices of public housing in Singapore. 

The **Geographical Weighted Regression (GWR)** method will be employed for calibrating this Hedonic Pricing Model.

## 1.2 The Data

We will be using the `Resale Flat Prices` data set from Data.gov.sg and focusing on:

- Transaction Prices of **Four-room Flats**;
- Within the transaction period of **1st January 2019 to 30th September 2020**

We will also be using the following data sets to evaluate the structural/locational factors:

- [Master Plan 2014 Subzone Boundary (Web) from Data.gov.sg](https://data.gov.sg/dataset/master-plan-2014-subzone-boundary-web)


# 2. Setting up R Enviroment

Before we get started, it is important for us to install/launch the necessary R packages into R environment.

The R packages we will be using are:

- R package for building OLS and performing diagnostics tests
    - [**olsrr**](https://olsrr.rsquaredacademy.com/index.html)
- R package for calibrating geographical weighted family of models
    - [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/index.html)
- R package for multivariate data visualisation and analysis
    - [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)
- Spatial data handling
    - **sf**
- Attribute data handling
    - **tidyverse**, especially **readr**, **ggplot2** and **dplyr**
- Choropleth mapping  
    - **tmap**
    
```{r}
packages = c('olsrr', 'corrplot', 'ggpubr', 'sf', 'spdep', 'GWmodel', 'tmap', 'tidyverse')
for (p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p,character.only = T)
}
```

The above code chunk will load the respective R packages into our R environment.

# 3. Geospatial Data Wrangling

## 3.1 Importing in the geospatial data

We will be using the `URA Master Plan 2014's planning subzone boundaries` data set, so as to compare the resale prices across the various town districts (e.g. CBD area vs Jurong East)

The code chunk below is used to import *MP_SUBZONE_WEB_PL* shapefile by using *st_read()* of **sf** packages.

```{r}
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")
```
*mpsz* will be stored as a simple feature object and the geometry type will be *multipolygon*.

## 3.2 Update CRS projection

The code chunk below updates the newly imported *mpsz* with the correct EPSG code (i.e. 3414).

```{r}
mpsz_svy21 <- st_transform(mpsz, 3414)
```

We will then verify the CRS is projected correctly.

```{r}
st_crs(mpsz_svy21)
```
Voila! The EPSG is shown to be **3414** now.

Next, we will now check the extent of *mpsz_svy21* by using *st_bbox()* of **sf** package.

```{r}
st_bbox(mpsz_svy21) #view extent
```
# 4. Aspatial Data Wrangling

## 4.1 Importing in the aspatial data

We will use *read_csv()* function of **readr** package to import `Resale Flat Prices` into R as a tibble data frame called *flat_resale*.

```{r}
flat_resale = read_csv("data/aspatial/resale-flat-prices(jan-2017-and-up).csv")
```
Next, we will do a check on the data structure using *glimpse()* to make sure the import was done correctly. 

```{r}
glimpse(flat_resale)
```
And great, the data imported seems fine. We shall proceed on.

## 4.2 Subsetting the data 

The following code chunk will use **Regular Expression (i.e. gsub)** and **String Concatenation (i.e. paste function)** concepts to convert `month` column to **date data type**.

```{r}
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
flat_resale$month <- trim(flat_resale$month)

flat_resale$month <- paste(flat_resale$month, "-01", sep="")
flat_resale$month <- as.Date(flat_resale$month, format = '%Y-%m-%d')
```

With the `month` column in date data type, we then can subset our dataframe within the chosen **Time Period (Jan 2019 to Sept 2020)** as well as only having only records of the transaction prices of **4-Room Flats**.

```{r}
# Within Time Period
flat_resale_subset <- flat_resale[flat_resale$month >= "2019-01-01" & flat_resale$month <= "2020-09-01", ]

# 4-Room Flat only
flat_resale_subset <- flat_resale_subset[flat_resale_subset$flat_type == "4 ROOM", ]
```

## 4.3 Geocoding the data records

Our dataframe does not have the coordinates required for our geospatial analysis. Therefore, we will be doing **geocoding** using the following steps.

Firstly, we will combine the columns `block` and `street_name` into a full address column named `address` (i.e. 406 + Ang Mo Kio Ave 10 = 406 Ang Mo Kio Ave 10) using *paste()* function. This process is essential as we will need the full address for the **OneMapSG Search API** to work properly later on.

```{r}
flat_resale_subset$address <- paste(flat_resale_subset$block, flat_resale_subset$street_name, sep=" ")
```

Various libraries/packages are imported in for this geocoding process:

- **OneMapSGapi** to use its Search API
- **httr** for requesting the API
- **dplyr** for *lapply()* function
- **jsonlite** for working with JSON 

### 4.3.1 Creating a custom function for requesting Search API

A custom function `getLatLng` is created to call the Search API, and the function *lapply()* is used to pass in the respective addresses as the search queries. The search results is then saved into variables `X` and `Y`, in text form instead of its raw JSON form.

```{r eval=FALSE}
library(onemapsgapi)
library(httr)
library(dplyr)
library(jsonlite)

getLatLng <- function(object) {
  base_url <- "https://developers.onemap.sg/commonapi/search?"

  my_results <- httr::GET(base_url, query = list(searchVal = object, returnGeom = "Y", getAddrDetails = "Y"))
  
  my_content <- httr::content(my_results, as='text')
  content_from_json <- jsonlite::fromJSON(my_content)
}

X <- lapply(flat_resale_subset$address, getLatLng)

Y <- lapply(X, `[`, "results")

# Need run twice cause its list in a list
Z <- do.call(rbind, Y)
Z <- do.call(rbind, Z)
```

![Screenshot of X](jsonresults.png)
*lapply()* is used again to get only the *results* portion from the lists in X, as the important information are found in *results* (refer to above image).

Finally, we will use *do.call(rbind)* to convert the list of lists in `Y` into a proper dataframe. The function is ran twice because there are nested lists in `Y`. 

```{r include=FALSE}
Z <- read_rds("data/rds/Z.rds")
```

### 4.3.2 Additional data cleaning

We will then further clean up the dataframe using the below code chunk. 

Essentially, we are subsetting only the necessary columns and removing non-essential duplicates in the dataframe `geo_df`.

```{r}
geo_df <- select(Z, BLK_NO, ROAD_NAME, BUILDING, ADDRESS, POSTAL, X, Y, LATITUDE, LONGITUDE)

geo_df$SEARCH <- paste(geo_df$BLK_NO, geo_df$ROAD_NAME)
geo_df <- select(geo_df, SEARCH, ADDRESS, POSTAL, X, Y, LATITUDE, LONGITUDE)

geo_df <- geo_df[!duplicated(geo_df$SEARCH),]
```

### 4.3.3 Dealing with inconsistency in Keys

Unexpectedly, I realised an issue when trying to combine `geo_df` with `flat_resale_subset`. The primary keys for joining the 2 dataframes together are slightly different (refer to image below). The addresses were **initially passed into the API using the short form addresses** as given by the original data set, however the **full addresses are passed back** into `geo_df` due to the inherent nature of the API.

![Screenshot of inconsistent keys](keys.png)
Therefore, there **wasn't a common key identifier** to join `flat_resale_subset` and `geo_df` together correctly.

The way to move on was to **change the actual names of the road lexicons (Avenues, Drive etc) to its short forms (Ave, Dr etc)**, or vice versa. I decided to stick with the former so as to line up with the original data set. 

The following code chunk is used to change the road lexicons to its respective shorter abbreviations.

```{r}
geo_df$CLEAN <- str_replace_all(geo_df$SEARCH, "AVENUE", "AVE")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "ROAD", "RD")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "SOUTH", "STH")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "NORTH", "NTH")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "STREET", "ST")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "CENTRAL", "CTRL")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "BUKIT", "BT")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "DRIVE", "DR")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "UPPER", "UPP")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "TANJONG", "TG")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "CRESCENT", "CRES")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "CLOSE", "CL")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "LORONG", "LOR")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "JALAN", "JLN")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "COMMONWEALTH", "C'WEALTH")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "PLACE", "PL")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "HEIGHTS", "HTS")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "GARDENS", "GDNS")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "TERRACE", "TER")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "PARK", "PK")
geo_df$CLEAN <- str_replace_all(geo_df$CLEAN, "KAMPONG", "KG")
```

After that was done, we can now join the 2 dataframes into one, named `flat_with_geo`.

```{r}
flat_with_geo <- left_join(flat_resale_subset, geo_df, by = c("address" = "CLEAN"))
```

A check was then made to see if there are any NA rows, in the case of the Search API being unable to detect their correct addresses/locations. 

```{r}
sum(is.na(flat_with_geo$SEARCH))
```
Unfortunately, we have **28 records** where the API had issues locating their Coordinates. **But on the brighter side, its only a small minority, with 28 NA rows out of ~15k data points.**

Using the below code chunk, we will then **omit them from the dataframe** since they do not have their coordinates tagged to them.

```{r}
flat_with_geo <- na.omit(flat_with_geo)

flat_sf <- flat_with_geo
```

## 4.4 Getting the respective locational factors 

The following location factors will be added to `flat_sf` for our analysis:

- Proximity to MRT Station
- Proximity to 1st Tier Primary School
- Proximity to Supermarket
- Proximity to CBD
- Number of Bus Stops within 350m
- Number of Childcare Centres within 350m

### 4.4.1 Proximity to MRT

Importing in the data for the point locations of MRT Stations, which can be downloaded from [Land Transport Authority Datamall]('https://datamall.lta.gov.sg/content/datamall/en.html').

```{r}
mrt_sf <- st_read(dsn = "data/geospatial/MRT", 
                layer = "MRTLRTStnPtt")
```
![](latlon.png)

Next, we will **convert the geometry coordinates into latitude and longitude coordinates** as shown by the screenshot above. The latitude and longitude will be added as separate columns.

We will use the following code chunk for the conversion process.

```{r}
mrt_sf$geometry2 <- st_transform(select(mrt_sf, geometry), 4326)

mrt_sf2 <- do.call(rbind, st_geometry(mrt_sf$geometry2)) %>% 
    as_tibble() %>% setNames(c("lon","lat"))

mrt_sf$lon <- mrt_sf2$lon
mrt_sf$lat <- mrt_sf2$lat

mrt_sf <- mrt_sf[,c("OBJECTID","STN_NAME","STN_NO","lon","lat","geometry")]
```

```{r}
head(mrt_sf)
```

Voila! Now we won't run into errors later on.

We will also convert the **LONGITUDE and LATITUDE** columns for `flat_with_geo` to *numeric* data type as they are currently in *character* data type, which is essential for running the **Origin-Destination Matrix** later on.

```{r}
flat_with_geo$LONGITUDE <- as.numeric(flat_with_geo$LONGITUDE)
flat_with_geo$LATITUDE <- as.numeric(flat_with_geo$LATITUDE)
```

The **regeos**, **data.table** and **geosphere** R packages will be imported and used to run the Origin-Destination Matrix, which will gives us the distance of **proximity to the nearest MRT Station** for each respective 4-room flat unit. 

Firstly, the Origin-Destination Matrix `odmatrix_mrt` is created using *CJ()* function from the **data.table** package, with the column names being updated using *names()* function of Base R. With `odmatrix_mrt`, it helps us to get **all combinations of Origin and Destination pairs.**

```{r}
library(rgeos)
library(data.table)
library(geosphere)

# Get all combinations of Origin and Destination Pairs
odmatrix_mrt <- CJ(flat_with_geo$ADDRESS, mrt_sf$STN_NAME)
names(odmatrix_mrt) <- c('Origin', 'Destination') 
```

Next, we will create **data.table** versions of `flat_with_geo` and `mrt_sf` for use in the process.

```{r}
# Changing to Data.Table type if not errors will occur later
flat_DT <- setDT(flat_with_geo)
mrt_DT <- setDT(mrt_sf)
```

The **coordinates of the Origins and Destinations will also be added in** using the following code chunk.

```{r}
# Add coordinates of Origins
odmatrix_mrt$lat_orig <- flat_DT$LATITUDE[match(odmatrix_mrt$Origin, flat_DT$ADDRESS)]
odmatrix_mrt$long_orig <- flat_DT$LONGITUDE[match(odmatrix_mrt$Origin, flat_DT$ADDRESS)]

# Add coordinates of Destinations
odmatrix_mrt$lat_dest <- mrt_DT$lat[match(odmatrix_mrt$Destination, mrt_DT$STN_NAME)]
odmatrix_mrt$long_dest <- mrt_DT$lon[match(odmatrix_mrt$Destination, mrt_DT$STN_NAME)]
```

Finally, we can calculate the distance between every Origin-Destination pair using *distHaversine()* function from **geosphere** package and then get the **nearest destinations for each respective origin** later on.

```{r}
# Calculate distances
odmatrix_mrt[ , dist := distHaversine(matrix(c(long_orig, lat_orig), ncol = 2),
                                    matrix(c(long_dest, lat_dest), ncol = 2))]

# And get the nearest destinations for each origin
nearest_mrt <- odmatrix_mrt[, .(Destination = Destination[which.min(dist)],
                    dist = min(dist)), by = Origin]
```

```{r}
head(nearest_mrt)
```

As shown by the output above, every flat unit will have the nearest MRT Station and the distance given in `nearest_mrt` dataframe, which will later be added into `flat_with_geo` dataframe as the **proximity to MRT Station**.

### 4.4.2 Proximity to 1st Tier Primary Schools

The second locational factor to be added is `Proximity to 1st Tier Primary Schools`.

Data and ranking of the primary schools is taken from the [SmileTutor website](https://smiletutor.sg/primary-school-ranking-choose-the-best-primary-schools-in-singapore/). 

The top 25 primary schools will be used for this study.

![](prischool-ss.png)

However the data does not have location attributes as shown in the screenshot above, so we will **use the OneMapSG Search API function** that we've created earlier on to get the location details.

The following code chunk imports in the data.

```{r}
pri_school = read_csv("data/aspatial/top-25-primary-schools.csv")
```

We will then use the OneMapSG Search API `getLatLng` function to get the location attributes for the schools.

```{r eval=FALSE}
getLatLng <- function(object) {
  base_url <- "https://developers.onemap.sg/commonapi/search?"

  my_results <- httr::GET(base_url, query = list(searchVal = object, returnGeom = "Y", getAddrDetails = "Y"))
  
  my_content <- httr::content(my_results, as='text')
  content_from_json <- jsonlite::fromJSON(my_content)
}

X <- lapply(pri_school$`School Name`, getLatLng)

Y <- lapply(X, `[`, "results")

# Need run twice cause its list in a list
school <- do.call(rbind, Y)
school <- do.call(rbind, school)

# Change rows to columns
school2 <- as.data.frame(t(school))
school <- as.data.frame(t(school2))
```

```{r include=FALSE}
school <- read_rds("data/rds/school.rds")
```

Changing the Lat Lon data types to numeric data type.

```{r}
school$LONGITUDE <- as.numeric(school$LONGITUDE)
school$LATITUDE <- as.numeric(school$LATITUDE)
```

The same Origin-Destination Matrix process as explained earlier on will be ran again, using the following code chunk.

```{r}
# Get all combinations of Origin and Destination Pairs
odmatrix_school <- CJ(flat_with_geo$ADDRESS, school$BUILDING)
names(odmatrix_school) <- c('Origin', 'Destination') 

# Changing to Data.Table type if not errors will occur later
flat_DT <- setDT(flat_with_geo)
school_DT <- setDT(school)

# Add coordinates of Origins
odmatrix_school$lat_orig <- flat_DT$LATITUDE[match(odmatrix_school$Origin, flat_DT$ADDRESS)]
odmatrix_school$long_orig <- flat_DT$LONGITUDE[match(odmatrix_school$Origin, flat_DT$ADDRESS)]

# Add coordinates of Destinations
odmatrix_school$lat_dest <- school_DT$LATITUDE[match(odmatrix_school$Destination, school_DT$BUILDING)]
odmatrix_school$long_dest <- school_DT$LONGITUDE[match(odmatrix_school$Destination, school_DT$BUILDING)]

# Calculate distances
odmatrix_school[ , dist := distHaversine(matrix(c(long_orig, lat_orig), ncol = 2),
                                    matrix(c(long_dest, lat_dest), ncol = 2))]

# And get the nearest destinations for each origin
nearest_school <- odmatrix_school[, .(Destination = Destination[which.min(dist)],
                    dist = min(dist)), by = Origin]
```

```{r}
head(nearest_school)
```

As shown by the output above, every flat unit will have the nearest 1st Tier Primary School and the distance given in `nearest_school` dataframe, which will later be added into `flat_with_geo` dataframe as the **proximity to 1st Tier Primary School**.

### 4.4.3 Proximity to Supermarket

The same process as explained by the earlier 2 factors will also be applied to the locations of **Supermarkets.**

```{r}
supermarket <- read_csv("data/aspatial/list-of-supermarket-licences.csv")
```

We will use the `getLatLng` function to get the locational attributes of the supermarkets.

One good thing about this dataset is that it has **postal codes** so the addresses will be easier to find.

```{r eval=FALSE}
getLatLng <- function(object) {
  base_url <- "https://developers.onemap.sg/commonapi/search?"

  my_results <- httr::GET(base_url, query = list(searchVal = object, returnGeom = "Y", getAddrDetails = "Y"))
  
  my_content <- httr::content(my_results, as='text')
  content_from_json <- jsonlite::fromJSON(my_content)
}

X <- lapply(supermarket$`postal_code`, getLatLng)

Y <- lapply(X, `[`, "results")

# Need run twice cause its list in a list
supermarket <- do.call(rbind, Y)
supermarket <- do.call(rbind, supermarket)

# Change rows to columns
supermarket2 <- as.data.frame(t(supermarket))
supermarket <- as.data.frame(t(supermarket2))
```

```{r include=FALSE}
supermarket <- read_rds("data/rds/supermarket.rds")
```

Same Origin-Destination Matrix process is used as shown earlier on, using the following code chunk.

```{r}
supermarket$LONGITUDE <- as.numeric(supermarket$LONGITUDE)
supermarket$LATITUDE <- as.numeric(supermarket$LATITUDE)
```

```{r}
# Get all combinations of Origin and Destination Pairs
odmatrix_supermarket <- CJ(flat_with_geo$ADDRESS, supermarket$SEARCHVAL)
names(odmatrix_supermarket) <- c('Origin', 'Destination') # update names of columns

# Changing to Data.Table type if not errors will occur later
flat_DT <- setDT(flat_with_geo)
supermarket_DT <- setDT(supermarket)

# Add coordinates of Origins
odmatrix_supermarket$lat_orig <- flat_DT$LATITUDE[match(odmatrix_supermarket$Origin, flat_DT$ADDRESS)]
odmatrix_supermarket$long_orig <- flat_DT$LONGITUDE[match(odmatrix_supermarket$Origin, flat_DT$ADDRESS)]

# Add coordinates of Destinations
odmatrix_supermarket$lat_dest <- supermarket_DT$LATITUDE[match(odmatrix_supermarket$Destination, supermarket_DT$SEARCHVAL)]
odmatrix_supermarket$long_dest <- supermarket_DT$LONGITUDE[match(odmatrix_supermarket$Destination, supermarket_DT$SEARCHVAL)]

# Calculate distances
odmatrix_supermarket[ , dist := distHaversine(matrix(c(long_orig, lat_orig), ncol = 2),
                                    matrix(c(long_dest, lat_dest), ncol = 2))]

# And get the nearest destinations for each origin
nearest_supermarket <- odmatrix_supermarket[, .(Destination = Destination[which.min(dist)],
                    dist = min(dist)), by = Origin]
```

```{r}
head(nearest_supermarket)
```

As shown by the output above, every flat unit will have the nearest supermarket and the distance given in `nearest_supermarker` dataframe, which will later be added into `flat_with_geo` dataframe as the **proximity to Supermarket**.

### 4.4.4 Proximity to CBD

The same process as explained by the earlier 3 factors will also be applied to the location of the **Central Business District of Singapore (CBD)**.

According to **Central Area, Singapore** wikipedia page, the **official coordinates of the CBD are 1°17′30″N 103°51′00″E.**

This converts to **1.291667, 103.85** when shown in decimal format, therefore we will use these coordinates for our Origin-Destination Matrix.

```{r}
CBD <- data.frame("ADDRESS" = character(0), "LATITUDE" = numeric(0), "LONGITUDE" = numeric(0))
CBD <- CBD %>% add_row(ADDRESS = "Central Business District", LATITUDE = 1.291667, LONGITUDE = 103.85)

CBD$LONGITUDE <- as.numeric(CBD$LONGITUDE)
CBD$LATITUDE <- as.numeric(CBD$LATITUDE)
```

Same Origin-Destination Matrix process is used as shown earlier on, using the following code chunk.

```{r}
# Get all combinations of Origin and Destination Pairs
odmatrix_CBD <- CJ(flat_with_geo$ADDRESS, CBD$ADDRESS)
names(odmatrix_CBD) <- c('Origin', 'Destination') 

# Changing to Data.Table type if not errors will occur later
flat_DT <- setDT(flat_with_geo)
CBD_DT <- setDT(CBD)

# Add coordinates of Origins
odmatrix_CBD$lat_orig <- flat_DT$LATITUDE[match(odmatrix_CBD$Origin, flat_DT$ADDRESS)]
odmatrix_CBD$long_orig <- flat_DT$LONGITUDE[match(odmatrix_CBD$Origin, flat_DT$ADDRESS)]

# Add coordinates of Destinations
odmatrix_CBD$lat_dest <- CBD_DT$LATITUDE[match(odmatrix_CBD$Destination, CBD_DT$ADDRESS)]
odmatrix_CBD$long_dest <- CBD_DT$LONGITUDE[match(odmatrix_CBD$Destination, CBD_DT$ADDRESS)]

# Calculate distances
odmatrix_CBD[ , dist := distHaversine(matrix(c(long_orig, lat_orig), ncol = 2),
                                    matrix(c(long_dest, lat_dest), ncol = 2))]

# And get the nearest destinations for each origin
nearest_CBD <- odmatrix_CBD[, .(Destination = Destination[which.min(dist)],
                    dist = min(dist)), by = Origin]
```

```{r}
head(nearest_CBD)
```

As shown by the output above, every flat unit will have their distance to the **CBD Area** given in the dataframe, which will later be added into `flat_with_geo` dataframe as the **proximity to CBD**.

### 4.4.5 Number of Bus Stops within 350M

The next locational factor will be slightly different as compared to the earlier 4, but the process is similar. The point location dataset of Bus Stops can be downloaded from [Land Transport Authority Datamall]('https://datamall.lta.gov.sg/content/datamall/en.html').

```{r}
BusStop <- st_read(dsn = "data/geospatial/BusStop", 
                layer = "BusStop")
```

Same as the MRT Stations, we will **convert the geometry coordinates into latitude and longitude coordinates**. 

```{r}
BusStop$geometry2 <- st_transform(select(BusStop, geometry), 4326)

BusStop2 <- do.call(rbind, st_geometry(BusStop$geometry2)) %>% 
    as_tibble() %>% setNames(c("lon","lat"))

BusStop$lon <- BusStop2$lon
BusStop$lat <- BusStop2$lat
```

<p> And running the same old Origin-Destination Matrix again &#128514; </p>

```{r eval=FALSE}
# Get all combinations of Origin and Destination Pairs
odmatrix_bus <- CJ(flat_with_geo$ADDRESS, BusStop$BUS_STOP_N)
names(odmatrix_bus) <- c('Origin', 'Destination') # update names of columns

# Changing to Data.Table type if not errors will occur later
flat_DT <- setDT(flat_with_geo)
bus_DT <- setDT(BusStop)

# Add coordinates of Origins
odmatrix_bus$lat_orig <- flat_DT$LATITUDE[match(odmatrix_bus$Origin, flat_DT$ADDRESS)]
odmatrix_bus$long_orig <- flat_DT$LONGITUDE[match(odmatrix_bus$Origin, flat_DT$ADDRESS)]

# Add coordinates of Destinations
odmatrix_bus$lat_dest <- bus_DT$lat[match(odmatrix_bus$Destination, BusStop$BUS_STOP_N)]
odmatrix_bus$long_dest <- bus_DT$lon[match(odmatrix_bus$Destination, BusStop$BUS_STOP_N)]

# Calculate distances
odmatrix_bus[ , dist := distHaversine(matrix(c(long_orig, lat_orig), ncol = 2),
                                    matrix(c(long_dest, lat_dest), ncol = 2))]
```

However theres a twist for the last step, we will be **subsetting distances less than 350m and counting them**later on.

```{r eval=FALSE}
# Subsetting only distances less than 350m
nearest_bus <- subset(odmatrix_bus, dist <= 350)
```

```{r include=FALSE}
# odmatrix_bus <- write_rds(odmatrix_bus, "data/rds/odmatrix_bus.rds")
# nearest_bus <- write_rds(nearest_bus, "data/rds/proximity_bus.rds")
```

```{r include=FALSE}
# odmatrix_bus <- read_rds("data/rds/odmatrix_bus.rds")
nearest_bus <- read_rds("data/rds/proximity_bus.rds")
```

```{r}
head(nearest_bus)
```

As shown above, this is what `nearest_bus` looks like, with **only records with distances less than 350M kept.**

Next, we will **remove duplicates** and **count the number of records left for each flat unit** using the following code chunk.

```{r}
library(dplyr)

nearest_bus <- nearest_bus %>%
               distinct(Destination, .keep_all = TRUE)

nearest_bus <- nearest_bus %>%
               group_by(Origin) %>%
               mutate(count = n())

head(nearest_bus)
```

As shown above, this is what `nearest_bus` looks like, with **with a `count` column that shows the number of bus stops within 350m** of a resale flat unit.

### 4.4.6 Number of Childcare Centres within 350M

For childcare centres locations, we will use the OneMapSG API to get the childcare data, as shown in the following code chunk.

```{r eval = FALSE}
library(onemapsgapi)

childcare <- get_theme('eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOjc5NTgsInVzZXJfaWQiOjc5NTgsImVtYWlsIjoianVucGVuZy50ZW8uMjAxOUBzbXUuZWR1LnNnIiwiZm9yZXZlciI6ZmFsc2UsImlzcyI6Imh0dHA6XC9cL29tMi5kZmUub25lbWFwLnNnXC9hcGlcL3YyXC91c2VyXC9zZXNzaW9uIiwiaWF0IjoxNjM1OTIzMTk5LCJleHAiOjE2MzYzNTUxOTksIm5iZiI6MTYzNTkyMzE5OSwianRpIjoiMjZlNGNkOWE5NzJiZDNiODU4OTczMWUyZTY2YWYzZDQifQ.HeD3nTXY94BQXhSauURz93nvsjj8aTmbzxv2taswOOk', 'childcare')
```

```{r include=FALSE}
childcare <- read_rds("data/rds/childcare.rds")
```

<p> And next you should already know it by now, running the same old Origin-Destination Matrix again &#128514; </p> 

```{r}
childcare$Lng <- as.numeric(childcare$Lng)
childcare$Lat <- as.numeric(childcare$Lat)
```

```{r eval=FALSE}
# Get all combinations of Origin and Destination Pairs
odmatrix_childcare <- CJ(flat_with_geo$ADDRESS, childcare$NAME)
names(odmatrix_childcare) <- c('Origin', 'Destination') # update names of columns

# Changing to Data.Table type if not errors will occur later
flat_DT <- setDT(flat_with_geo)
childcare_DT <- setDT(childcare)

# Add coordinates of Origins
odmatrix_childcare$lat_orig <- flat_DT$LATITUDE[match(odmatrix_childcare$Origin, flat_DT$ADDRESS)]
odmatrix_childcare$long_orig <- flat_DT$LONGITUDE[match(odmatrix_childcare$Origin, flat_DT$ADDRESS)]

# Add coordinates of Destinations
odmatrix_childcare$lat_dest <- childcare_DT$Lat[match(odmatrix_childcare$Destination, childcare$NAME)]
odmatrix_childcare$long_dest <- childcare_DT$Lng[match(odmatrix_childcare$Destination, childcare$NAME)]

# Calculate distances
odmatrix_childcare[ , dist := distHaversine(matrix(c(long_orig, lat_orig), ncol = 2),
                                    matrix(c(long_dest, lat_dest), ncol = 2))]

# And get the nearest destinations for each origin
nearest_childcare <- subset(odmatrix_childcare, dist <= 350)
```

```{r include=FALSE}
# nearest_childcare <- write_rds(nearest_childcare, "data/rds/nearest_childcare.rds")
```

```{r include=FALSE}
nearest_childcare <- read_rds("data/rds/nearest_childcare.rds")
```

Next, we will **remove duplicates** and **count the number of records left for each flat unit** using the following code chunk.

```{r}
library(dplyr)

nearest_childcare <- nearest_childcare %>%
               distinct(Destination, .keep_all = TRUE)

nearest_childcare <- nearest_childcare %>%
                     group_by(Origin) %>%
                     mutate(count = n())

head(nearest_childcare)
```

As shown above, this is what `nearest_childcare` looks like, with **with a `count` column that shows the number of childcare centres within 350m** of a resale flat unit.

## 4.5 Adding the locational factors to flat_with_geo

Now, we will add the respective locational factors into `flat_with_geo` dataframe.

First, we need to change unit of measurement from metres to kilometres using the following code chunk.

```{r}
nearest_mrt$dist <- (nearest_mrt$dist / 1000)
nearest_school$dist <- (nearest_school$dist / 1000)
nearest_supermarket$dist <- (nearest_supermarket$dist / 1000)
nearest_CBD$dist <- (nearest_CBD$dist / 1000)
```

Then, we will Change the column names to how they are classified (Proximity to MRT etc). 

```{r}
names(nearest_mrt)[names(nearest_mrt) == "dist"] <- "Proximity to MRT"
names(nearest_school)[names(nearest_school) == "dist"] <- "Proximity to 1st Tier Primary School"
names(nearest_supermarket)[names(nearest_supermarket) == "dist"] <- "Proximity to Supermarket"
names(nearest_CBD)[names(nearest_CBD) == "dist"] <- "Proximity to CBD"
names(nearest_bus)[names(nearest_bus) == "count"] <- "No. of Bus Stops within 350m"
names(nearest_childcare)[names(nearest_childcare) == "count"] <- "No. of Childcare Centres within 350m"
```

```{r include=FALSE}
flat_with_geo <- flat_sf
```

Then we will add the respective fields into `flat_with_geo` using *left_join()*.

```{r}
flat_with_geo <- left_join(flat_with_geo, nearest_mrt, by = c("ADDRESS" = "Origin"))
flat_with_geo <- left_join(flat_with_geo, nearest_school, by = c("ADDRESS" = "Origin"))
flat_with_geo <- left_join(flat_with_geo, nearest_supermarket, by = c("ADDRESS" = "Origin"))
flat_with_geo <- left_join(flat_with_geo, nearest_CBD, by = c("ADDRESS" = "Origin"))
flat_with_geo <- left_join(flat_with_geo, nearest_bus, by = c("ADDRESS" = "Origin"))
flat_with_geo <- left_join(flat_with_geo, nearest_childcare, by = c("ADDRESS" = "Origin"))
```

Next, we will **subset only the essential columns** and change all **column names into uppercase.**

```{r}
flat_with_geo <- flat_with_geo[c("month", "ADDRESS", "LATITUDE", "LONGITUDE", "POSTAL", "resale_price", "storey_range","floor_area_sqm", "remaining_lease", "Proximity to MRT", "Proximity to 1st Tier Primary School", "Proximity to Supermarket", "Proximity to CBD", "No. of Bus Stops within 350m", "No. of Childcare Centres within 350m")]
names(flat_with_geo) <- toupper(names(flat_with_geo))
```

And then **replace all NA fields with "0" value**, as this would only apply to `No. of Bus Stops within 350` and `No. of Childcare Centres within 350m`.

Having a value of 0 means that there are no facility within the stated distance of 350m. 

```{r}
flat_with_geo[is.na(flat_with_geo)] <- 0

flat_with_geo$`NO. OF BUS STOPS WITHIN 350M` <- as.numeric(flat_with_geo$`NO. OF BUS STOPS WITHIN 350M`)
flat_with_geo$`NO. OF CHILDCARE CENTRES WITHIN 350M` <- as.numeric(flat_with_geo$`NO. OF CHILDCARE CENTRES WITHIN 350M`)
```

We will also change the columns `STOREY_RANGE` and `REMAINING_LEASE` to their numeric counterparts through the code chunk below.

For `STOREY_RANGE`, I will use the lower limit (e.g. 01 TO 02, 01 will be used instead).

For `REMAINING_LEASE`, only the year will be retained (e.g. 61 years 06 months -> 61).

```{r}
flat_with_geo$STOREY_RANGE <- gsub(" .*$", "", flat_with_geo$STOREY_RANGE) %>%
                              as.numeric(flat_with_geo$STOREY_RANGE)

flat_with_geo$REMAINING_LEASE <- gsub(" .*$", "", flat_with_geo$REMAINING_LEASE) %>%
                              as.numeric(flat_with_geo$REMAINING_LEASE)
```

```{r}
glimpse(flat_with_geo)
```

Finally, we have the completed data set as shown by the output above. 

Great! It took a really long time to reach this step, let us now move on.


## 4.6 Converting aspatial data frame into a sf object

Currently, `flat_with_geo` data frame is aspatial. We will convert it to a sf object. 

The code chunk below converts `flat_with_geo` into a simple feature data frame by using *st_as_sf()* of **sf** package, then *st_transform()* is used to convert the coordinates from WGS84 (crs=4326) to SVY21 (crs=3414).

```{r}
flat_resale.sf <- st_as_sf(flat_with_geo,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
                  st_transform(crs=3414)

head(flat_resale.sf)
```
# 5. Exploratory Data Analysis

## 5.1 Statistical Graphics

We will plot the distribution of **resale_price** by using **ggplot** using the code chunk below.

```{r}
ggplot(flat_resale.sf, aes(x=`RESALE_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```
The figure above reveals a relatively right skewed distribution. This means that more public housing units were transacted at relative lower prices.

Statistically, the skewed distribution can be normalised by using log transformation, if needed to. The code chunk below is used to derive a new variable called **log_resale_price** by using a log transformation on the variable **resale_price**. 

The transformation is performed using *mutate()* of **dplyr** package.

```{r}
flat_resale.sf <- flat_resale.sf %>%
  mutate(`LOG_RESALE_PRICE` = log(RESALE_PRICE))
```

We will then plot the distribution of **log_resale_price** using the below code chunk.

```{r}
ggplot(data=flat_resale.sf, aes(x=`LOG_RESALE_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```
The distribution became less skewed after the transformation, however this is just an example. For this study we will still be using `RESALE_PRICE` in the later steps.

## 5.2 Drawing Statistical Point Map

Lastly, we want to reveal the geospatial distribution of public housing resale prices in Singapore. 

The map will be prepared by using **tmap** package with their interactive mode.

```{r fig.width=12, fig.height=8}
tmap_mode("view")

tm_shape(mpsz_svy21)+
  tm_polygons() +
tm_shape(flat_resale.sf) +  
  tm_dots(col = "RESALE_PRICE",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

# 6. Hedonic Pricing Modelling 

We will now build hedonic pricing models for public housing resale units using *lm()* of Base R.

## 6.1 Simple Linear Regression Method

Firstly, we will test out a simple linear regression model by using the most anticipated independent variable, which is the square area of the unit `FLOOR_AREA_SQM`. 

This is because most people would think its logical the bigger the property, the higher its resale price right? Therefore, let's build a model and find out.

`RESALE_PRICE` will be the dependent variable while `FLOOR_AREA_SQM` will be the independent variable in the model.

```{r}
flat.slr <- lm(formula=RESALE_PRICE ~ FLOOR_AREA_SQM, data = flat_resale.sf)
```

*lm()* returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).

We will then use the functions *summary()* and *anova()* to obtain and print a summary and analysis of the results. 

```{r}
summary(flat.slr)
```
The output report reveals that the SELLING_PRICE can be explained by using the formula:

*RESALE_PRICE = 556616 + -1190(FLOOR_AREA_SQM)*

The R-squared of 0.004657 reveals that the simple regression model built is only able to explain a very small portion of the resale prices.

However, since p-value is much smaller than 0.0001, we will reject the null hypothesis that the mean is a good estimator of RESALE_PRICE. This will allow us to infer that the simple linear regression model above is a relatively good estimator of RESALE_PRICE.

The **Coefficients:** section of the report reveals that the p-values of both the estimates of the Intercept and FLOOR_AREA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a result, we will be able to infer that the B0 and B1 are good parameter estimates.

To visualise the best fit curve on a scatterplot, we can then incorporate *lm()* as a method function in ggplot’s geometry as shown in the code chunk below.

```{r}
ggplot(data=flat_resale.sf,  
       aes(x=`FLOOR_AREA_SQM`, y=`RESALE_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm)
```

Unexpectedly, the figure above actually reveals that the model is poorly fitted as there is high variability, and the values of FLOOR_AREA_SQM hardly fits into a straight line. 

Therefore we shall conclude that **FLOOR_AREA_SQM is not a good indicator of predicting the RESALE_PRICE.**

## 6.2 Multiple Linear Regression Method

### 6.2.1 Visualising the relationships of the respective independent variables

Instead of doing multiple simple regression models with the various independent variables, we will build a Multiple Regression model instead, which is more time efficient. 

This allows for predictions of the dependent variable `RESALE_PRICE` alongside multiple independent variables at one go.

The first step is to ensure the independent variables used are not highly correlated to each other, as **multicollinearity will affect the quality of a model.** 

We will use a Correlation Matrix to visualise the relationships between the independent variables, by using **corrplot** package in the code chunk below.

```{r fig.width=12, fig.height=8}
corrplot(cor(flat_with_geo[, 7:13]), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

According to this [article by Clairvoyant](https://blog.clairvoyantsoft.com/correlation-and-collinearity-how-they-can-make-or-break-a-model-9135fbe6936a), a data and decision engineering company, the presence of **Muliticollinearity** is indicated when the **correlation coefficient is > 0.7.** 

Since no coefficient values exceed the threshold as seen in the scatterplot matrix, we will use all independent variables in the modelling building.

### 7.2.2 Building a hedonic pricing model using multiple linear regression method

The code chunk below using *lm()* to calibrate the multiple linear regression model.

```{r}
flat.mlr <- lm(formula = RESALE_PRICE ~ STOREY_RANGE + FLOOR_AREA_SQM + REMAINING_LEASE + `PROXIMITY TO MRT` + `PROXIMITY TO 1ST TIER PRIMARY SCHOOL` + `PROXIMITY TO SUPERMARKET` + `PROXIMITY TO CBD` + `NO. OF BUS STOPS WITHIN 350M`  + `NO. OF CHILDCARE CENTRES WITHIN 350M`, data=flat_resale.sf)

summary(flat.mlr)
```
With reference to the report above, we can see that majority of the independent variables are statistically significant. Only `PROXIMITY TO SUPERMARKET` AND `NO. OF CHILDCARE CENTRES WITHIN 350M` is not, so we will revise the model by removing them.

Let us calibrate the revised model by using the following code chunk.

```{r}
flat.mlr1 <- lm(formula = RESALE_PRICE ~ STOREY_RANGE + FLOOR_AREA_SQM + REMAINING_LEASE + `PROXIMITY TO MRT` + `PROXIMITY TO 1ST TIER PRIMARY SCHOOL` + `PROXIMITY TO CBD` + `NO. OF BUS STOPS WITHIN 350M`, data=flat_resale.sf)

ols_regress(flat.mlr1)
```
### 6.2.3 Additional check for multicollinearity 

Another alternative method will be used to ensure there are no multicollinearity between the independent variables.

The *ols_vif_tol()* function of **olsrr** package will be used, however there are some errors with the function so I will just leave the code chunk here for now.

```{r eval=FALSE}
ols_vif_tol(flat.mlr1)
```

### 6.2.4 Test for Non-Linearity

It is important to test whether linearity and additivity exists in the relationship between dependent and independent variables.

We will use the *ols_plot_resid_fit()* of **olsrr** package to perform the linearity assumption test.

```{r}
ols_plot_resid_fit(flat.mlr1)
```
The figure above reveals that most of the data points are relatively scattered around the 0 line, hence it is **safe to conclude that the relationship between the dependent variable and the respective independent variables are linear.**

### 6.2.5 Test for Normality Assumption

Lastly, we will do a Normality Assumption Test using *ols_plot_reside_hist()* function.

```{r}
ols_plot_resid_hist(flat.mlr1)
```
The figure reveals that the residual of the multiple linear regression model (i.e. flat.mlr1) resembles a normal distribution.

### 6.2.6 Testing for Spatial Autocorrelation
The hedonic model we are building uses geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.

In order to perform spatial autocorrelation test, we will need to convert `flat_resale.sf` into a SpatialPointsDataFrame.

First, we will export the residual of the hedonic pricing model and save it as a data frame.

```{r}
mlr.output <- as.data.frame(flat.mlr1$residuals)
```

Next, we will join the newly created data frame with `flat_resale.sf` object.

```{r}
flat_resale.res.sf <- cbind(flat_resale.sf, 
                        flat.mlr1$residuals) %>%
rename(`MLR_RES` = `flat.mlr1.residuals`)
```

Next, we will convert `flat_resale.res.sf` simple feature object into a SpatialPointsDataFrame because **spdep** package can only process sp conformed spatial data objects.

The code chunk below will be used to perform the data conversion process.

```{r}
flat_resale.sp <- as_Spatial(flat_resale.res.sf)
flat_resale.sp
```
Next, we will use **tmap** package to visualise the distribution of the residuals on an interactive map.

```{r fig.width=12, fig.height=8}
tmap_mode("view")

tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.4) +
tm_shape(flat_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

The figure above reveals **signs of spatial correlation.**

Therefore, we will perform a **Moran's I Test**. The first step will be computing a distance-based weight matrix by using *dnearneigh()* function of **spdep** package.

```{r}
nb <- dnearneigh(coordinates(flat_resale.sp), 0, 1500, longlat = FALSE)
summary(nb)
```
Next, *nb2listw()* will be used to convert the output neighbours lists (i.e. nb) into spatial weights.

```{r}
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```
Next, *lm.morantest()* will be used to perform Moran’s I test for residual spatial autocorrelation.

```{r}
lm.morantest(flat.mlr1, nb_lw)
```
The Global Moran’s I test for residual spatial autocorrelation shows that it’s **p-value is less than 0.00000000000000022** which is less than the alpha value of 0.05. Hence, **we will reject the null hypothesis that the residuals are randomly distributed.**

Since the Observed Global Moran I = 0.4672783 which is greater than 0, **we can infer that the residuals resembles cluster distribution.**

# 7. Building Hedonic Pricing Models using GWmodel

We will build Hedonic Pricing Models using **both the fixed and adaptive bandwidth** schemes.

## 7.1 Building Fixed Bandwidth GWR Model

### 7.1.1 Computing fixed bandwith

The code chunk below uses the function *bw.gwr()* of GWModel package to determine the optimal fixed bandwidth to use in the model. 

The argument **adaptive** is set to **FALSE** as we are interested in using the fixed bandwidth scheme.

```{r eval=FALSE}
bw.fixed <- bw.gwr(formula = RESALE_PRICE ~ STOREY_RANGE + FLOOR_AREA_SQM + REMAINING_LEASE + `PROXIMITY.TO.MRT` + `PROXIMITY.TO.1ST.TIER.PRIMARY.SCHOOL` + `PROXIMITY.TO.CBD` + `NO..OF.BUS.STOPS.WITHIN.350M`  + `NO..OF.CHILDCARE.CENTRES.WITHIN.350M`, data=flat_resale.sp, approach="CV", kernel="gaussian", adaptive=FALSE, longlat=FALSE)
```

![](fixedbw.png)    

```{r include=FALSE}
# bw.fixed <- write_rds(bw.fixed, "data/rds/bw.fixed.rds")
# gwr.fixed <- write_rds(gwr.fixed, "data/rds/gwr.fixed.rds")
```

```{r include=FALSE}
bw.fixed <- read_rds("data/rds/bw.fixed.rds")
gwr.fixed <- read_rds("data/rds/gwr.fixed.rds")
```

The output result shows that the **recommended bandwidth is around 242.41 metres.** 

Hence, we will then calculate the local regression using all the data points within this recommended bandwidth.

### 7.1.2 GWModel method - fixed bandwith

Now we can calibrate the GWR model using fixed bandwidth and gaussian kernel.

```{r eval=FALSE}
gwr.fixed <- gwr.basic(formula = RESALE_PRICE ~ STOREY_RANGE + FLOOR_AREA_SQM + REMAINING_LEASE + `PROXIMITY.TO.MRT` + `PROXIMITY.TO.1ST.TIER.PRIMARY.SCHOOL` + `PROXIMITY.TO.CBD` + `NO..OF.BUS.STOPS.WITHIN.350M`  + `NO..OF.CHILDCARE.CENTRES.WITHIN.350M`, data=flat_resale.sp, bw=bw.fixed, kernel = 'gaussian', longlat = FALSE)
```

The output is saved in a list of class "gwrm", and let us see the output using the codechunk below.

```{r}
gwr.fixed
```

The report shows that the **adjusted r-square of the gwr is 0.975144 which is significantly better than the global multiple linear regression model of 0.7179 .**

## 7.2 Building Adaptive Bandwidth GWR Model

### 7.2.1 Computing adaptive bandwidth

Similar to the earlier section, we will first use *bw.ger()* to determine the recommended data point to use.

The code chunk used look very similar to the one used to compute the fixed bandwidth except the **adaptive** argument will be changed to **TRUE.**

```{r eval=FALSE}
bw.adaptive <- bw.gwr(formula = RESALE_PRICE ~ STOREY_RANGE + FLOOR_AREA_SQM + REMAINING_LEASE + `PROXIMITY.TO.MRT` + `PROXIMITY.TO.1ST.TIER.PRIMARY.SCHOOL` + `PROXIMITY.TO.CBD` + `NO..OF.BUS.STOPS.WITHIN.350M`  + `NO..OF.CHILDCARE.CENTRES.WITHIN.350M`, data=flat_resale.sp, approach="CV", kernel="gaussian", adaptive=TRUE, longlat=FALSE)
```
![](adaptbw.png)

```{r include=FALSE}
# bw.adaptive <- write_rds(bw.adaptive, "data/rds/bw.adaptive.rds")
# gwr.adaptive <- write_rds(gwr.adaptive, "data/rds/gwr.adaptive.rds")
```

```{r include=FALSE}
bw.adaptive <- read_rds("data/rds/bw.adaptive.rds")
gwr.adaptive <- read_rds("data/rds/gwr.adaptive.rds")
```

The output result shows that **251 is the recommended data point to be used**.

Hence, we will then calculate the local regression using all the data points within this recommended bandwidth.

### 6.2.2 Computing the adaptive bandwidth gwr model

Now, we can go ahead with calibrating the gwr-based hedonic pricing model by using **adaptive bandwidth and gaussian kernel** as shown in the code chunk below.

```{r eval=FALSE}
gwr.adaptive <- gwr.basic(formula = RESALE_PRICE ~ STOREY_RANGE + FLOOR_AREA_SQM + REMAINING_LEASE + `PROXIMITY.TO.MRT` + `PROXIMITY.TO.1ST.TIER.PRIMARY.SCHOOL` + `PROXIMITY.TO.CBD` + `NO..OF.BUS.STOPS.WITHIN.350M`  + `NO..OF.CHILDCARE.CENTRES.WITHIN.350M`, data=flat_resale.sp, bw=bw.adaptive, kernel = 'gaussian', adaptive=TRUE, longlat = FALSE)
```

We will then look at the model output using the below code chunk.

```{r}
gwr.adaptive
```
The report shows that the **adjusted r-square of the gwr is 0.94727 which is significantly better than the global multiple linear regression model of 0.7179 .**

# 8. Visualising GWR Output

In addition to regression residuals, the output feature class table includes fields for: 

- **Condition Number:** this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30 may be unreliable.

- **Local R2:** these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate that the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.

- **Predicted:** these are the estimated (or fitted) y values 3. computed by GWR.

- **Residuals:** to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produced by using these values.

- **Coefficient Standard Error:** these values measures the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.

They are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called **SDF** of the output list.

## 8.1 Converting SDF into sf data frame

To visualise the fields in SDF, we need to first convert it into sf dataframe by using the code chunk below.

```{r}
flat_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs=3414)

flat_resale.sf.adaptive.svy21 <- st_transform(flat_resale.sf.adaptive, 3414)

gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)
flat_resale.sf.adaptive <- cbind(flat_resale.res.sf, as.matrix(gwr.adaptive.output))
```

```{r}
glimpse(flat_resale.sf.adaptive)
```

```{r}
summary(gwr.adaptive$SDF$yhat)
```

## 8.2 Visualising local R2

We will create an interactive map to **visualise the local R2** results.

```{r fig.width=12, fig.height=8}
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(flat_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))
```
Results:

The local R2 values range between 0.0 to 1.0 and they indicate how well the local regression model fits observed y-values, which are the factors affecting the `RESALE_PRICE` of public housing resale flats. Very low value for the local R2 indicates that the local model is performing poorly. Hence, those with higher R squared values are of better prediction than those that are lower. **From the plot above, we can find that the local R2 values range from 0.7 to 1.0.** 

The strongest intensity in orange tone (indicating higher local R2) happens in a few areas: **1) Central Region, 2) Far West and 3) North.** Thus, we can infer that those regions has areas that show that the factors analysed in the study fits the local regression model very well as compared to other regions. To conclude, **the relationships between the factors and the `RESALE_PRICE` was much better captured by the model in those areas.**

For the remaining areas, **the relationships between the factors and the `RESALE_PRICE` are captured slightly weaker than the areas mentioned above. However, the model still fits relatively well as the lowest value is 0.7.**

In essence, the model is performing well as a whole, as the values are relatively high. 

## 8.3 Visualising Coefficient Standard Error (Intercept_SE)

```{r fig.width=12, fig.height=8}
tmap_mode("view")
tm_shape(mpsz_svy21) + 
  tm_polygons(alpha = 0.1) + 
tm_shape(flat_resale.sf.adaptive) +
  tm_dots(col="Intercept_SE", 
          border.col="gray60",
          border.lwd=1) +
  tm_view(set.zoom.limits = c(11,14))
```
Results:

These values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.

As seen in the output above, all of the values fall in the **0 mln to 20 mln** range, which is the smallest. **This means that our built model has no issues with local collinearity.**

That's great!

# 9. Conclusion

Finally, we have reached the end of the study, let us do a brief summary again.

In this study, the following affecting factors were studied and analysed:

- Storey Range
- Floor Area (Sqm)
- Remaining Lease
- Proximity to MRT Station
- Proximity to 1st Tier Primary School
- Proximity to Supermarket
- Proximity to CBD
- Number of Bus Stops within 350m
- Number of Childcare Centres within 350m

As seen in the above analyses, the majority of the factors was found to affect the resale prices of housing units, only **PROXIMITY TO SUPERMARKET** and **NO. OF CHILDCARE CENTRES WITHIN 350M** does not. 

**Therefore, the confirmed list of factors that does affect the resale prices of housing units are:**

- Storey Range
- Floor Area (Sqm)
- Remaining Lease
- Proximity to MRT Station
- Proximity to 1st Tier Primary School
- Proximity to CBD
- Number of Bus Stops within 350m

<p> As such, now you know what factors to look out for when searching for a resale unit that is within your budget &#128516;<p/>

Thank you for looking through this study and hope this helps in your future property search!

# 10. References

- [How to get API data with R](https://www.youtube.com/watch?v=tlaJf0CHbFE&t=370s&ab_channel=IDGTECHtalk)
- [Passing many values to an API using R](https://stackoverflow.com/questions/61534474/passing-many-values-to-an-api-using-r)
- [OneMapSG Search API Documentation](https://www.onemap.gov.sg/docs/#search)
- [How to find nearest using Origin-Destination Matrix](https://stackoverflow.com/questions/44608687/how-to-find-the-nearest-distance-between-two-different-data-frames)



